{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/10 18:25:41 WARN Utils: Your hostname, codespaces-7c15f5 resolves to a loopback address: 127.0.0.1; using 10.0.2.28 instead (on interface eth0)\n",
      "24/08/10 18:25:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/codespace/.ivy2/cache\n",
      "The jars for the packages stored in: /home/codespace/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/python/3.10.13/lib/python3.10/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "net.snowflake#spark-snowflake_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-735ea9fa-6533-4abf-90cf-ea2e2db397d0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound net.snowflake#spark-snowflake_2.12;2.10.0-spark_3.2 in central\n",
      "\tfound net.snowflake#snowflake-ingest-sdk;0.10.3 in central\n",
      "\tfound net.snowflake#snowflake-jdbc;3.13.14 in central\n",
      "downloading https://repo1.maven.org/maven2/net/snowflake/spark-snowflake_2.12/2.10.0-spark_3.2/spark-snowflake_2.12-2.10.0-spark_3.2.jar ...\n",
      "\t[SUCCESSFUL ] net.snowflake#spark-snowflake_2.12;2.10.0-spark_3.2!spark-snowflake_2.12.jar (1166ms)\n",
      "downloading https://repo1.maven.org/maven2/net/snowflake/snowflake-ingest-sdk/0.10.3/snowflake-ingest-sdk-0.10.3.jar ...\n",
      "\t[SUCCESSFUL ] net.snowflake#snowflake-ingest-sdk;0.10.3!snowflake-ingest-sdk.jar (1012ms)\n",
      "downloading https://repo1.maven.org/maven2/net/snowflake/snowflake-jdbc/3.13.14/snowflake-jdbc-3.13.14.jar ...\n",
      "\t[SUCCESSFUL ] net.snowflake#snowflake-jdbc;3.13.14!snowflake-jdbc.jar (2246ms)\n",
      ":: resolution report :: resolve 5009ms :: artifacts dl 4431ms\n",
      "\t:: modules in use:\n",
      "\tnet.snowflake#snowflake-ingest-sdk;0.10.3 from central in [default]\n",
      "\tnet.snowflake#snowflake-jdbc;3.13.14 from central in [default]\n",
      "\tnet.snowflake#spark-snowflake_2.12;2.10.0-spark_3.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-735ea9fa-6533-4abf-90cf-ea2e2db397d0\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (35916kB/62ms)\n",
      "24/08/10 18:25:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://110c8954-3756-428f-a803-594e5749476e.internal.cloudapp.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Snowflake to PySpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7bf0fc4ba170>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install snowflake-connector-python\n",
    "# pip install load_dotenv\n",
    "\n",
    "import os\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Snowflake to PySpark\")\n",
    "    .config(\"spark.jars.packages\", \"net.snowflake:spark-snowflake_2.12:2.10.0-spark_3.2\") \n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate())\n",
    "\n",
    "# Display the Spark session \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/10 18:26:12 WARN SnowflakeConnectorUtils$: Query pushdown is not supported because you are using Spark 3.0.1 with a connector designed to support Spark 3.2. Either use the version of Spark supported by the connector or install a version of the connector that supports your version of Spark.\n"
     ]
    }
   ],
   "source": [
    "# Snowflake connection options\n",
    "sfOptions = {\n",
    "    \"sfURL\": f\"https://{os.getenv('ACCOUNT')}.snowflakecomputing.com\",\n",
    "    \"sfUser\": os.getenv('USER_NAME'),\n",
    "    \"sfPassword\": os.getenv('PASSWORD'),\n",
    "    \"sfDatabase\": \"TEST\",\n",
    "    \"sfSchema\": \"TEST\",\n",
    "    \"sfWarehouse\": \"compute_wh\",\n",
    "    \"sfRole\": \"ACCOUNTADMIN\"\n",
    "}\n",
    "\n",
    "# Read the data from Snowflake table into a DataFrame\n",
    "df = spark.read \\\n",
    "    .format(\"net.snowflake.spark.snowflake\") \\\n",
    "    .options(**sfOptions) \\\n",
    "    .option(\"dbtable\", \"COVID19_EPIDEMIOLOGICAL_DATA.PUBLIC.DEMOGRAPHICS\") \\\n",
    "    .load()\n",
    "\n",
    "# Register the DataFrame as a temporary SQL view for processing\n",
    "df.createOrReplaceTempView(\"demographics\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----+----------+------------+-----+-----------------+----------------+---------------------+-----------------------+\n",
      "|ISO3166_1|ISO3166_2| FIPS|  LATITUDE|   LONGITUDE|STATE|           COUNTY|TOTAL_POPULATION|TOTAL_MALE_POPULATION|TOTAL_FEMALE_POPULATION|\n",
      "+---------+---------+-----+----------+------------+-----+-----------------+----------------+---------------------+-----------------------+\n",
      "|       US|       TX|48203|32.5036285| -94.0554582|   TX|  Harrison County|           66431|                32573|                  33858|\n",
      "|       US|       TX|48205|35.7167664|-102.3805857|   TX|   Hartley County|            5966|                 3596|                   2370|\n",
      "|       US|       TX|48207| 33.120371| -99.6317724|   TX|   Haskell County|            5812|                 3236|                   2576|\n",
      "|       US|       TX|48209|30.0241697| -97.7473311|   TX|      Hays County|          185686|                92221|                  93465|\n",
      "|       US|       TX|48211|35.8159564|-100.2792102|   TX|  Hemphill County|            4151|                 1985|                   2166|\n",
      "|       US|       TX|48213|32.3228236| -95.4845036|   TX| Henderson County|           79213|                38586|                  40627|\n",
      "|       US|       TX|48215|26.2047339| -97.8825424|   TX|   Hidalgo County|          828334|               404079|                 424255|\n",
      "|       US|       TX|48217| 31.841747| -96.8037638|   TX|      Hill County|           34901|                17216|                  17685|\n",
      "|       US|       TX|48219|33.6334228|-102.1706769|   TX|   Hockley County|           23377|                11525|                  11852|\n",
      "|       US|       TX|48221|32.3663108| -97.6426745|   TX|      Hood County|           54217|                26724|                  27493|\n",
      "|       US|       TX|48223|33.1515579| -95.3256014|   TX|   Hopkins County|           35844|                17592|                  18252|\n",
      "|       US|       TX|48225|31.3982961| -95.0987768|   TX|   Houston County|           22802|                12225|                  10577|\n",
      "|       US|       TX|48227|32.3500086|-101.2469093|   TX|    Howard County|           36423|                20654|                  15769|\n",
      "|       US|       TX|48229|31.8359049|-105.0529571|   TX|  Hudspeth County|            3481|                 1745|                   1736|\n",
      "|       US|       TX|48231|33.2485932| -95.8719606|   TX|      Hunt County|           89068|                43952|                  45116|\n",
      "|       US|       TX|48233|35.7271773|-101.2485912|   TX|Hutchinson County|           21782|                10853|                  10929|\n",
      "|       US|       TX|48235|31.2601562|-100.8225712|   TX|     Irion County|            1631|                  838|                    793|\n",
      "|       US|       TX|48237| 33.087966| -98.0606191|   TX|      Jack County|            8866|                 5037|                   3829|\n",
      "|       US|       TX|48239|28.8247197| -96.3911982|   TX|   Jackson County|           14678|                 7199|                   7479|\n",
      "|       US|       TX|48241|30.6621449| -93.8909795|   TX|    Jasper County|           35640|                17406|                  18234|\n",
      "+---------+---------+-----+----------+------------+-----+-----------------+----------------+---------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3140"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/10 16:29:28 WARN SnowflakeConnectorUtils$: Query pushdown is not supported because you are using Spark 3.0.1 with a connector designed to support Spark 3.2. Either use the version of Spark supported by the connector or install a version of the connector that supports your version of Spark.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/10 16:29:32 WARN SnowflakeConnectorUtils$: Query pushdown is not supported because you are using Spark 3.0.1 with a connector designed to support Spark 3.2. Either use the version of Spark supported by the connector or install a version of the connector that supports your version of Spark.\n"
     ]
    }
   ],
   "source": [
    "df.write.format(\"snowflake\") \\\n",
    "    .options(**sfOptions) \\\n",
    "    .option(\"dbtable\", \"TEST.TEST.DEMOGRAPHICS_TEST\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/10 16:31:14 WARN SnowflakeConnectorUtils$: Query pushdown is not supported because you are using Spark 3.0.1 with a connector designed to support Spark 3.2. Either use the version of Spark supported by the connector or install a version of the connector that supports your version of Spark.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3140"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read \\\n",
    "    .format(\"net.snowflake.spark.snowflake\") \\\n",
    "    .options(**sfOptions) \\\n",
    "    .option(\"dbtable\", \"TEST.TEST.DEMOGRAPHICS_TEST\") \\\n",
    "    .load().count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
